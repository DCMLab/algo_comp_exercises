{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressing-manufacturer",
   "metadata": {},
   "source": [
    "## Using Earsketch for placing audio samples on a temporal grid\n",
    "\n",
    "**Given:** a list of up to 10 samples and a trigger map mapping an onset to one of the samples. Every onset stops the playback of a previous sample that might still be playing (several layers can be created that way if you want samples to overlap)\n",
    "\n",
    "**Goal:** an audio file that contains the samples at the desired positions.\n",
    "\n",
    "This process can be used to plug together your entire composition, or for producing new samples that can then be used in the next iteration.\n",
    "\n",
    "### 1. Upload samples to Earsketch\n",
    "\n",
    "* Head to https://earsketch.gatech.edu/earsketch2/\n",
    "* In order to upload, you need to create an account.\n",
    "* On the left side, open the \"Sounds\" tab and click \"Add sound\". You should see the upload mask:\n",
    "\n",
    "<img src=\"earsketch_upload.png\">\n",
    "\n",
    "* under \"Constant Value\" add a meaningful name. Note that it will be capitalized and your username will be prepended.\n",
    "* Scroll down in the sound collection until you see the category with your username. Here you find the names of all your uploaded samples which you can use as variables in the Python code on the right.\n",
    "\n",
    "### Combine samples and triggermap\n",
    "\n",
    "<img src=\"earsketch_playback.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-sheriff",
   "metadata": {},
   "source": [
    "* In the code field, the first three lines and the last one always need to be there. We set the tempo to 60 bpm, so that every beat has the duration of 1 second.\n",
    "* Then we define the list with the `samples` that will be triggered and note the list index of each.\n",
    "* Now we define a triggermap `onsets`:\n",
    "  * every position in the string corresponds to 1/4 of a beat, so 0.25 seconds is the lowest resolution (in this tempo)\n",
    "  * an integer 0-9 causes the sample at this index to be played back\n",
    "  * every `+` causes the playback to continue, every `-` stands for a rest\n",
    "* then we combine the `samples` list with the `onsets`, using the function `makeBeat()`. The arguments are:\n",
    "  * a single sample or list of samples\n",
    "  * the track (layer) in which the samples are to be played\n",
    "  * the measure in which to begin the playback\n",
    "  * the triggerlist\n",
    "* once defined, we can hit the green \"Run\" button and see the outcome on top in the Digital Audio Workstation (DAW)\n",
    "* in the example, you see that\n",
    "  * the triggermap `onsets` contains 1 for the second sample, `JEYES_GOAT` and 0 for the first sample, `JEYES_HARM`\n",
    "  * it is put into the first track at measure 2\n",
    "  * the second call to the `makeBeat` function only receives one sample and therefore only uses 0 in the triggermap, which also contains rests, i.e, for `0-`, only the first 0.25 seconds of the sample are being played.\n",
    "  \n",
    "Here are a few lines of code to give you an idea how you can translate your durations expressed in seconds to such a triggermap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exciting-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0----1++++++0+++++++++++++++++++++++++++'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def duration2earsketch(dur, tempo=60):\n",
    "    \"\"\"For a duration in seconds and a tempo, return the number of symbols to put into the trigger map.\"\"\"\n",
    "    resolution = 4 * round(tempo) / 60\n",
    "    return round(float(dur) * resolution)\n",
    "\n",
    "def earsketch_event(sample_no, positions, sounding_positions=None):\n",
    "    \"\"\"Pass the index of the sample to be played, the number you got from duration2earsketch,\n",
    "    and how many of the positions you want the sample to keep sounding (the rest is filled up with rests).\"\"\"\n",
    "    if sample_no is None:\n",
    "        return '-' * positions\n",
    "    positions -= 1\n",
    "    sounding_positions = positions if sounding_positions is None else sounding_positions - 1\n",
    "    rest_positions = positions - sounding_positions\n",
    "    return f\"{sample_no}{'+' * sounding_positions}{'-' * rest_positions}\"\n",
    "\n",
    "def durations2triggermap(succession_of_samples, onsets, tempo=60):\n",
    "    if isinstance(succession_of_samples, int):\n",
    "        succession_of_samples = [succession_of_samples]\n",
    "    nxt_sample = (s for s in succession_of_samples)\n",
    "    result = ''\n",
    "    for ons in onsets:\n",
    "        positions = duration2earsketch(ons)\n",
    "        if isinstance(ons, str):\n",
    "            result += earsketch_event(None, positions)\n",
    "        else:\n",
    "            try:\n",
    "                sample_no = next(nxt_sample)\n",
    "            except:\n",
    "                break\n",
    "            result += earsketch_event(sample_no, positions)\n",
    "    return result\n",
    "            \n",
    "sample_succession = [0, 1, 0]\n",
    "durations_in_seconds = [0, '1', 1.75, 6.9]    \n",
    "durations2triggermap(sample_succession, durations_in_seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:algo]",
   "language": "python",
   "name": "conda-env-algo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
